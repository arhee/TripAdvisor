{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TripAdvisor Recommender Model Building (under construction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "<ul>\n",
    "\n",
    "<li>Tourist attractions are governed by power law-like distributions</li>\n",
    "<li>The sparseness, lack of rating variance, and low user participation of this partial TripAdvisor dataset does not lend itself well to a recommender system. Systems with user stickiness (Netflix, Amazon) where user feedback can be rapidly captured would likely be more amenable</li>\n",
    "<li>A simple implementation of a matrix factorization algorithm (SVD) achieves a 3% better score over predicting the average of the attraction</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Having Tripadvisor recommend a lot of temples, I wanted to see if I could build a better recommendation engine within the computational limits of my laptop.\n",
    "\n",
    "<b>Data</b>\n",
    "<ul>\n",
    "<li>430k reviews, 200k users, 5k reviewed items at 800MB.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Processing</b>\n",
    "<ul>\n",
    "<li>Scraped, cleaned, and processed with Python</li>\n",
    "<li>Stored in SQLite</li>\n",
    "<li>Visualizations done in R</li>\n",
    "</ul>\n",
    "\n",
    "Analysis on the dataset available <a href=\"\"> here</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Characterization\n",
    "\n",
    "#### Very sparse.\n",
    "\n",
    "Immediately, we notice that the dataset is extremely sparse (0.03%).  In comparison, in 2006, when Netflix held a competition to improve their recommendation algorithm, the sparsity of that dataset was 1%, a 30x difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Everything is awesome! High average ratings\n",
    "\n",
    "A histogram of the tripadvisor ratings show that it is skewed towards the upper end.  Tourists are very satisfied.  The average rating is a high 4.4/5 stars.  This is in contrast to the Yelp data set with an average of 3.7/5 stars and the  <a href=\"http://www.timelydevelopment.com/demos/NetflixPrize.aspx\">Netflix data</a>, where the average rating was 3.5/5 stars and roughly normally distributed.  \n",
    "\n",
    "<img src=\"figs/ratings_histo.png\" style=\"max-height: 400; max-width: 600px;\">\n",
    "\n",
    "Intuitively, this makes sense.  Movies incur a one time upfront cost for production.  Distribution costs of digital goods are neglible and shelf space isn't an issue, so Netflix can house an almost infinite selection. Therefore, a bad movie on Netflix can live in perpetuity.\n",
    "\n",
    "In contrast, tourist attractions are dynamic. If tourists dislike an attraction, the operators will eventually go out of business; thus, there is a positive selection bias in the attractions listed on TripAdvisor.  One would probably observe a similar effect in industries where the customer is looking for a one-time great experience - amusement parks, safaris, etc. How common are mediocre amusement parks?\n",
    "\n",
    "Restaurants are also dynamic. However, customers may not always prioritize ratings but optimize on other dimensions such as convenience and affordability (fast food).  Therefore, restaurants may not face a strong selection pressure based on ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power laws: few attractions get most of the reviews, few users do most of the reviewing\n",
    "\n",
    "By plotting the distribution of reviews for attractions, we find that it is governed by a power law-like distribution with a very long tail.  A large number of attractions have a handful of reviews while few attractions have a large number of reviews. \n",
    "<img src=\"figs/reviews_item_histo.png\" style=\"max-height: 400; max-width: 600px;\">\n",
    "\n",
    "One plausible explanation is that when dealing with popularity, there is a positive feedback loop, popular items become more popular.  If you have a week of vacation, you are likely to spend it visiting the most popular attraction than the 50th most popular.  This phenomenon of preferential attachment is widely known under many names such as Zipf's law, the Pareto principle, and the Matthew effect.\n",
    "\n",
    "<img src=\"figs/reviews_user_histo.png\" style=\"max-height: 400; max-width: 600px;\">\n",
    "\n",
    "We also find that in this dataset, users overwhelmingly only rate a single attraction.  There is noticeable a lack of stickiness; users don't keep rating attractions, it seems to be a one-off endeavour.\n",
    "\n",
    "Therefore, the evidence suggests that the properties of the dataset are a predictable function of the structure of TripAdvisor (non-stickiness) and the tourism industry (preferential attachment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Framework\n",
    "\n",
    "Despite the possible difficulties, I was curious to see if algorithms could beat the baseline predictor of the attraction average.\n",
    "\n",
    "The cost function was set as the RMSE.  Models were trained using k-folds cross-validation with 5 folds, and the final score was obtained by averaging the RMSE over the five folds. Although it would have been more rigorous to conduct a nested k-folds optimization to prevent over-fitting, for simplicity and exploratory purposes, the entire dataset was used in the training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "Classification models including SVD, logistic regression, and random forests were found to have high errors during an exploratory search and were discluded from further optimization.\n",
    "\n",
    "#### Linear models\n",
    "\n",
    "A number of features and 2-way interaction features were extracted from the dataset.  Features were selected based on a greedy selection process.  L2 regularization was used.\n",
    "\n",
    "#### Simple Model\n",
    "\n",
    "The algorithms for the simple and SVD models are detailed in a  <a href=\"http://public.research.att.com/~volinsky/netflix/kdd08koren.pdf\">paper</a> written for the Netflix competition.  Both models were implemented in python, optimized using stochastic gradient descent with L2 regularization utilizing the <a href=\"http://sifter.org/~simon/journal/20061211.html\">Simon Funk algorithm</a>.  All parameters were optimized using exhaustive grid search.\n",
    "\n",
    "The algorithms are briefly described below:\n",
    "\n",
    "An estimate of the rating $\\hat{r}_{u,i}$, by user $u$ for item $i$ can be modeled with simple biases for both the user $b_{u}$ and the item $b_{i}$:\n",
    "$$ \\hat{r}_{u,i} = \\mu + b_{u} + b_{i}, $$\n",
    "\n",
    "where $\\mu$ is the overall average rating.  The bias parameters are estimated by optimizing the corresponding cost function.\n",
    "#### Singular Value Decomposition (SVD)\n",
    "\n",
    "With SVD, the goal is to uncover latent features that explain the dataset.  Each user $u$ is associated with a vector $p_{u}$ and each item with an items-factor vector $q_{i}$.  The inner product of the vectors is then the estimate of the rating:\n",
    "\n",
    "$$\\hat{r}_{u,i} = q_{i}^{T}p_{u}.$$\n",
    "\n",
    "#### Improved SVD\n",
    "Improved SVD is a blend of both the simple model and SVD:\n",
    "\n",
    "$$ \\hat{r}_{u,i} = q_{i}^{T}p_{u} +  \\mu + b_{u} + b_{i}.$$\n",
    "\n",
    "Therefore, the final combined cost function to be optimized is as follows:\n",
    "\n",
    "$$\\min_{b_{*}, p_{*}, q_{*}}\\sum_{(u,i)\\in\\kappa}({r}_{u,i} - \\mu - b_{u} - b_{i} - q_{i}^{T}p_{u})^2 + \\lambda(b_{u}^2 + b_{i}^2 + \\lVert q_{i}\\rVert^2 + \\lVert p_{u} \\rVert^2),$$\n",
    "\n",
    "where $\\kappa$ is the set of all the reviews, $\\lambda$ is the learning rate constant, and $r_{u,i}$ is the rating given by user $u$ for item $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "<!-- <img src=\"figs/model_performance.png\" style=\"max-height: 400; max-width: 600px;\"> -->\n",
    "\n",
    "\n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <td><b>Model</b></td>\n",
    "    <td><b>RMSE</b></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Simple Model + SVD</td>\n",
    "    <td>0.783</td>\n",
    "  </tr>\n",
    "<tr>\n",
    "    <td>Attraction Average [<font color='green'>baseline</font>]</td>\n",
    "    <td>0.782</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>Linear Model</td>\n",
    "    <td>0.781</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Simple Model</td>\n",
    "    <td>0.763</td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td><font color='blue'>SVD</font></td>\n",
    "    <td><font color='blue'>0.761</font></td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "We find that SVD model fares best with a modest 3% improvement over the baseline.\n",
    "\n",
    "Interestingly, the blend of baseline predictors and SVD did not fare as well as either of the individual predictors. In addition, only a handful of features: average attraction rating, average user rating; improved the predictive capabilities of the linear model.  Surprisingly, there were no benefits from second-order terms combining temporal data and location data.\n",
    "\n",
    "Future steps could possibly include incorporating information from the reviews into the prediction using LDA as demonstrated <a href=\"http://www.yelp.com/html/pdf/YelpDatasetChallengeWinner_HiddenFactors.pdf\">here</a>, blending the different models, or implementing nearest-neighbor features.\n",
    "\n",
    "However, it's debatable whether further model optimization is worthwhile.  Given that the 97% of users rate < 5 items and the previously mentioned difficulties of the dataset, there is the inevitably of diminishing returns to effort; it is difficult to predict a user's preferences absent a rich history.\n",
    "\n",
    "From a product perspective, it may be more productive in either improving user 'stickiness' or providing a seamless way to extract a new user's preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "By utilizing matrix factorization techniques, we are able to improve upon the baseline predictor.  However, after reviewing the collected TripAdvisor dataset, there seems to be limited gains from implementing a recommender system based purely on ratings. \n",
    "\n",
    "The dataset is very rich in reviews and can be mined for insights into idiosyncratic characteristics of tourist attractions.  To that end, a better product involving this dataset may be a search function designed to look for locations that aren't categorized into preset TripAdvisor categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
